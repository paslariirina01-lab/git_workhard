{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paslariirina01-lab/git_workhard/blob/main/%D0%9A%D0%BE%D0%BF%D0%B8%D1%8F_%D0%B1%D0%BB%D0%BE%D0%BA%D0%BD%D0%BE%D1%82%D0%B0_%D0%9F%D0%B0%D1%81%D0%BB%D0%B0%D1%80%D1%8C_%D0%98%D1%80%D0%B8%D0%BD%D0%B0__%22hw_tokenization__ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiXdJjBa9sCq"
      },
      "source": [
        "# Домашнее задание: Токенизация текста"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дан список текстов, которые нужно токенизировать разными способами"
      ],
      "metadata": {
        "id": "1xVbvaj_phyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\n",
        "\"The quick brown fox jumps over the lazy dog. It's a beautiful day!\",\n",
        "\"Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\",\n",
        "\"I can't believe she's going! Let's meet at Jane's house. They'll love it.\",\n",
        "\"What's the ETA for the package? Please e-mail support@example.com ASAP!\"\n",
        "]"
      ],
      "metadata": {
        "id": "uj-xaNnwpiPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Используйте способы токенизации, которые описаны в практикуме. Каждый способ нужно обернуть в функцию, например:\n",
        "\n",
        " ```python\n",
        " def simple_tokenization(string):\n",
        "   return string.split()\n",
        "   ```"
      ],
      "metadata": {
        "id": "ix1Im4Kcqb3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Напишите функцию для токенизации по пробелам и знакам препинания (используйте оператор `def`)"
      ],
      "metadata": {
        "id": "Ih0BBOGBpv6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ваш код здесь\n",
        "import re\n",
        "\n",
        "def simple_tokenization(string):\n",
        "\n",
        "    try:\n",
        "        tokens = re.findall(r\"\\w+|[^\\w\\s]\", string)\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при токенизации: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "W1QCaw6cqDnn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Напишите функцию для токенизации текста с помощью NLTK"
      ],
      "metadata": {
        "id": "GThvPcovqgO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ваш код здесь\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def nltk_tokenization(string):\n",
        "\n",
        "    try:\n",
        "        return word_tokenize(string)\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка NLTK: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "14BIv33iqrkL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de6672f7-94a3-4599-8d0c-61951cb88b47"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишите функцию для токенизации текста с помощью Spacy"
      ],
      "metadata": {
        "id": "GxW7ZP6iqwpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ваш код здесь\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def spacy_tokenization(string):\n",
        "\n",
        "    try:\n",
        "        doc = nlp(string)\n",
        "        return [token.text for token in doc]\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка spaCy: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "B0NQg-VfuFW_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. С помощью цикла `for` примените каждую из написанных функций к каждому тексту из списка `texts`"
      ],
      "metadata": {
        "id": "WmyJfB9wuKkm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lvUmk94MhrL8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "be392b37-a220-4e4b-a0b8-4098c8386936"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Простая токенизация =====\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'texts' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2467109751.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n===== {name} =====\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nТекст {i}: {text}\\nТокены: {tokens}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'texts' is not defined"
          ]
        }
      ],
      "source": [
        "# Ваш код здесь\n",
        "tokenizers = {\"Простая токенизация\": simple_tokenization, \"NLTK\": nltk_tokenization, \"spaCy\": spacy_tokenization}\n",
        "\n",
        "for name, func in tokenizers.items():\n",
        "    print(f\"\\n===== {name} =====\")\n",
        "    for i, text in enumerate(texts, start=1):\n",
        "        tokens = func(text)\n",
        "        print(f\"\\nТекст {i}: {text}\\nТокены: {tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqAgf6sGhrL8"
      },
      "source": [
        "##### Критерии оценки (макс. балл == 5):\n",
        "\n",
        "- Функциональность (до 4 баллов)): Все методы работают корректно (запускаем код, и он работает)\n",
        "- Качество кода (до 1 балла): Чистый, документированный код с обработкой ошибок (кратко описать, что вы дополнили самостоятельно, например, \"добавлена токенизация `spacy`\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теоретические вопросы (макс. балл == 5; в ведомость выставляется сумма за практику и теорию)\n",
        "\n",
        "Необходимо дать краткие ответы на вопросы по теме \"токенизация\". В сумме длина ответов на вопрос не должна превышать размер вордовской страницы 14 шрифтом."
      ],
      "metadata": {
        "id": "Mwe1Co6MvibX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Почему простое разделение текста по пробелам и знакам препинания часто является недостаточным для современных NLP-задач? Приведите 2-3 конкретных примера, когда деление текста по символам или словам не работает. (2 балла за полный и подробный ответ на вопрос)\n",
        "\n",
        "2. Сколько токенов во фразе \"You shall know a word by the company it keeps\" в модели GPT-5? Как вы получили это значение? (1 балл за правильный ответ и ссылку на ресурс, с помощью которого вы узнали эту информацию)\n",
        "\n",
        "3. Опишите своими словами работу алгоритма BPE (можно форматировать ответ с использованием списков, 2 балла за корректное описание и ясное изложение ответа)"
      ],
      "metadata": {
        "id": "mgE2bQFXv0MG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Простое разделение по пробелам и знакам препинания не отражает реальную структуру языка. Современные NLP-модели требуют более тонкой сегментации для правильного анализа смысла и контекста. Например: когда используются сокращения и апострофы. Например, фраза “I’m doing” может быть ошибочно разделена на “I” и “m”, хотя правильная форма — “I am”.\n",
        "Помимо этого,проблема может возникнуть при наличии таких имен как: “Mrs. Smith” простая токенизация может ошибочно разделить “Mrs.” на “Mrs” и “.”\n",
        "\n",
        "2. Во фразе: You shall know a word by the company it keeps - 10 токенов. https://gpt-tokenizer.dev/\n",
        "\n",
        "3. Byte Pair Encoding (BPE) - помогает уменьшить количество уникальных токенов в тексте, постепенно объединяя часто встречающиеся символы или группы символов в более крупные единицы — подслова.\n",
        "Он ищет повторяющиеся комбинации символов и заменяет их новым токеном. Таким образом, частые слова остаются целыми, а редкие разбиваются на части.\n",
        "\n",
        "1.Алгоритм видит все символы как отдельные токены.\n",
        "2.Подсчёт пар.\n",
        "3.Алгоритм считает, какие пары символов встречаются чаще всего.\n",
        "4.Объединение частых пар.\n",
        "5.Самая частая пара объединяется в один токен.\n",
        "6.Повторение шага.\n",
        "7.Снова считает пары и объединяет самую частую.\n",
        "8.Продолжение до нужного размера словаря.\n",
        "9.Объединяет дальше пока не получит окончательный вид словаря."
      ],
      "metadata": {
        "id": "ph6OrDeL_m41"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}